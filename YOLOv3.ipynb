{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object Detection on Video inputs Using YOLOv3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Having examined the performance of SSD300 on images, we now extend our analysis of object detection to video sequences. The motivation for this work can lead to the following use cases:**\n",
    "\n",
    "- **Tracking suspicious activities in a crowded field. For example, the model can be retrained to capture instances of backpacks such as the one used by the Boston Bomber**\n",
    "- **Self driving cars**\n",
    "- **Object detection for video searching: Rather than searching for videos based on file names, AI based searches can search for videos based on objects in the video**\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to YOLOv3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Yolo stands for \"You Only Look Once\" v3. It is an improvement over SSD (1). The diagram below shows that YOLOV3-spp results in a mean Average Precision (mAP) OF 60.6 compared to the mean Average Precision (map) of 41.2 for SSD300**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='imgs/yolov3-perfomance.png' alt='Yolo Comparison' style=\"height: 400px;width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Yolo v3 Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='imgs/yolov3-artichecture.png' alt='yolov3-artichecture'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bounding Box Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**YoloV3 predicts tx, ty, tw, th as indicated in the diagram below**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='imgs/bound_box.png' alt='Yolo Comparison' style=\"height: 400px;width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Class Prediction\n",
    "**Each bounding box in Yolov3 predicts a class that an object belongs to using multilabel classification using independent logistic classifier and not softmax**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extractor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Yolov3 uses the Darknet-53 architecture shown below to extract features of the inputs. This architecture consists of 53 layer convolutional network. The darknet-53 uses pretrained weights obtained from Imagenet dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='imgs/darknet-53.png' alt='Yolo Comparison'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non maximum suppression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use non maximum suppression to ensure that a class in only identified one. Suppose we have an image of 100X100 and a grid of 9X9 grid. If this truck below fallls in  multiple cells of grid, non maximum suppression ensures we identify the optimal cell from from the all the cells that the truck in detected in.\n",
    "\n",
    "nmsThreshold\n",
    "\n",
    "Non maximum suppression:\n",
    "1. Discards cells where probability of class being present is <= nmsThreshold\n",
    "2. Use the  cell with greatedprobability among candidates for object as a prediction\n",
    "3. Last discard any remaining cell with Intersection over union value >= confThreshold "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='imgs/non-max-suppression.png' alt='non-max-suppression.png'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Yolov3 Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We will examine the performance of YoloV3 against a video input. OpenCV4 will be the library that we will use to read and write videos as needed**\n",
    "\n",
    "**For this project, we will reuse pretrained weights obtained by training the model against Cocodata sets [2]. We went with this approach because we found out training the model from scratch for only a few epochs took several days even with AWS p3.2xlarge resources**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2 #import openCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from tqdm import tqdm_notebook, tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OpenCV4 provides a convinient method for loading model and weights** [3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_config, model_weights):\n",
    "    model = cv2.dnn.readNetFromDarknet(model_config, model_weights) #load the model and weights\n",
    "    model.setPreferableBackend(cv2.dnn.DNN_BACKEND_OPENCV) # Set Open CY as backend\n",
    "    model.setPreferableTarget(cv2.dnn.DNN_TARGET_CPU) # Set target CPU\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to return the names of the output layers of the model\n",
    "def getModelOutputLayers(model):    \n",
    "    layerNames= model.getLayerNames() ## Get the list names of all the layers in the network\n",
    "    # getUnconnectedOutLayers() returns the numpy array of index of the layer that is not connected. \n",
    "    #A layer that is not connected is equilavent to the last year\n",
    "    #pick the index return by getUnconnectedOutLayers() and pass them to layerNames to get the laste\n",
    "    namesModelOutputLayers = [layerNames[i[0] - 1] for i in model.getUnconnectedOutLayers()]\n",
    "    return namesModelOutputLayers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step to understanding YOLO is how it encodes its output. The input image is divided into an S x S grid of cells. For each object that is present on the image, one grid cell is said to be “responsible” for predicting it. That is the cell where the center of the object falls into.\n",
    "\n",
    "Each grid cell predicts B bounding boxes as well as C class probabilities. The bounding box prediction has 5 components: (x, y, w, h, confidence). The (x, y) coordinates represent the center of the box, relative to the grid cell location (remember that, if the center of the box does not fall inside the grid cell, than this cell is not responsible for it). These coordinates are normalized to fall between 0 and 1. The (w, h) box dimensions are also normalized to [0, 1], relative to the image size. Let’s look at an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drawBoundaryBox(classId, conf, left, top, right, bottom, frame):\n",
    "    # Draw a bounding box.\n",
    "    cv2.rectangle(frame, (left, top), (right, bottom), (255, 178, 50), 3)\n",
    "    \n",
    "    label = '%.2f' % conf        \n",
    "    # Get the label for the class name and its confidence\n",
    "    if classes:\n",
    "        assert(classId < len(classes))\n",
    "        label = '%s:%s' % (classes[classId], label)\n",
    "\n",
    "    #Display the label at the top of the bounding box\n",
    "    labelSize, baseLine = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)\n",
    "    top = max(top, labelSize[1])\n",
    "    cv2.rectangle(frame, (left, top - round(1.5*labelSize[1])), (left + round(1.5*labelSize[0]), top + baseLine), (255, 255, 255), cv2.FILLED)\n",
    "    cv2.putText(frame, label, (left, top), cv2.FONT_HERSHEY_SIMPLEX, 0.75, (0,0,0), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formally we define confidence as Pr(Object) * IOU(pred, truth) . If no object exists in that cell, the confidence score should be zero. Otherwise we want the confidence score to equal the intersection over union (IOU) between the predicted box and the ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_boundary(frame, detection):\n",
    "    dimension = {}\n",
    "    fHeight = frame.shape[0]\n",
    "    fWidth = frame.shape[1]\n",
    "    \n",
    "    Cx = int(detection[0] * fWidth)\n",
    "    Cy = int(detection[1] * fHeight)\n",
    "    \n",
    "    width = int(detection[2] * fWidth)\n",
    "    height = int(detection[3] * fHeight)\n",
    "    left = int(Cx - width / 2)\n",
    "    top = int(Cy - height / 2)\n",
    "    boundary = [left, top, width, height ]\n",
    "    return  boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess(frame, finalLayerOutputs, score_threshold, nms_threshold):\n",
    "    information = []\n",
    "    frameHeight = frame.shape[0]\n",
    "    frameWidth = frame.shape[1]\n",
    "\n",
    "    # For allthe bounding boxes output in the model, save boxes where we have high confidence scores\n",
    "    # the class with the highest score i assigned as the box classs\n",
    "    classIds = []\n",
    "    scores = []\n",
    "    bboxes = []\n",
    "    for finalLayerOutput in finalLayerOutputs:\n",
    "        for detection in finalLayerOutput:\n",
    "            detectionScores = detection[5:] #get the \n",
    "            classId = np.argmax(detectionScores) #Get the index of the highest score\n",
    "            confidence = detectionScores[classId]  #Detemine the confidence level of that score\n",
    "            if confidence >score_threshold:\n",
    "                boundary  = get_boundary(frame, detection)               \n",
    "                classIds.append(classId)\n",
    "                scores.append(float(confidence))\n",
    "                bboxes.append(boundary)\n",
    "\n",
    "    # Perform non maximum suppression to eliminate redundant overlapping boxes with lower confidences.    \n",
    "    indices = cv2.dnn.NMSBoxes(bboxes, scores, score_threshold, nms_threshold)\n",
    "    for i in indices:\n",
    "        i = i[0]\n",
    "        box = bboxes[i]\n",
    "        left = box[0]\n",
    "        top = box[1]\n",
    "        width = box[2]\n",
    "        height = box[3]\n",
    "        drawBoundaryBox(classIds[i], scores[i], left, top, left + width, top + height, frame)\n",
    "        information.append(classIds[i])\n",
    "    return information       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Preprocess input frame:\n",
    "#This function scales, crops,resizes an image input\n",
    "def preprocesFrame(frame,  scalefactor, size, mean, crop=False):\n",
    "    processedFrame  = cv2.dnn.blobFromImage(\n",
    "                                image = frame ,  #input frame\n",
    "                                scalefactor= scalefactor, #normalize\n",
    "                                size = size, #resize \n",
    "                                mean = mean, #mean subtraction value from (R, G, B),        \n",
    "                                crop= crop)\n",
    "    return processedFrame "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using OpenCV to read and write Videos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_object(model, imgWidth,imgHeight, fpath_originalVideo,fpath_detectionVideo, score_threshold, nms_threshold):\n",
    "    \n",
    "    '''\n",
    "    imgWidth = Image Width of the input frame. Since we are using Darknet-53, this should be 416  \n",
    "    imgLength = Image Length of the input frame. Since we are using Darknet-53 this should be 416  \n",
    "    fpath_originalVideo = /path/to/original_video\n",
    "    fpath_detectionVideo = /path/to/detection_video\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    log_scene = [] #Store scene information in this variable\n",
    "    \n",
    "    videoCapture = cv2.VideoCapture(fpath_originalVideo) #capture the originalVideo and  process it frame-by-frame \n",
    "    \n",
    "    #Define video write parameters\n",
    "    # saves the video in Motion JPEG video formart. See: https://www.fourcc.org/mjpg/\n",
    "    #fourcc = cv2.VideoWriter_fourcc('M','J','P','G')\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'MJPG')\n",
    "    fps = 30 #set frames per second rate\n",
    "    imgW=  round(videoCapture.get(cv2.CAP_PROP_FRAME_WIDTH))  #get current image Width \n",
    "    imgH = round(videoCapture .get(cv2.CAP_PROP_FRAME_HEIGHT)) #get current image Height \n",
    "       \n",
    "    \n",
    "                 \n",
    "    videoWriter= cv2.VideoWriter(filename = fpath_detectionVideo,# name of file to be written\n",
    "                                            fourcc = fourcc, # vide0 format\n",
    "                                            fps = fps, #write speeed in frames per sec\n",
    "                                            frameSize = (imgW, imgH)\n",
    "                                            )   \n",
    " \n",
    "    \n",
    "    '''\n",
    "    vid_writer = cv2.VideoWriter(fpath_detectionVideo, cv2.VideoWriter_fourcc('M','J','P','G'), 30, \n",
    "                             (round(cap.get(cv2.CAP_PROP_FRAME_WIDTH)),round(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))))\n",
    "    '''\n",
    "    while cv2.waitKey(1) < 0:        \n",
    "     \n",
    "        # read current video frame\n",
    "        hasFrame, frame = videoCapture.read()\n",
    "\n",
    "        # If the videoCapture does not capture any frames, we have reached the end of the video\n",
    "        # stop the detection process \n",
    "        if not hasFrame:           \n",
    "            print('Object Detection Completed for: ', fpath_detectionVideo)\n",
    "            cv2.waitKey(3000)\n",
    "            break\n",
    "\n",
    "        # preprocess frames\n",
    "        processedFrame = preprocesFrame(frame = frame,  \n",
    "                                        scalefactor =  1/255, #normalize\n",
    "                                        size  = (imgWidth, imgHeight), # we are not resizing\n",
    "                                        mean = [0,0,0] #We are not making a change\n",
    "                                       )\n",
    "\n",
    "\n",
    "        # Set the input \n",
    "        model.setInput(processedFrame)\n",
    "\n",
    "        # Get the names of output layers \n",
    "        namesModelOutputLayers = getModelOutputLayers(model)\n",
    "        \n",
    "        #get the output of the final layers\n",
    "        finalLayerOutputs = model.forward(namesModelOutputLayers)\n",
    "\n",
    "        # get and draw boundaries, as well as scene information\n",
    "       \n",
    "        information = postprocess(frame, finalLayerOutputs, score_threshold, nms_threshold)\n",
    "\n",
    "        # Put efficiency information. The function getPerfProfile returns the \n",
    "        # overall time for inference(t) and the timings for each of the layers(in layersTimes)\n",
    "        retval, _ = model.getPerfProfile()\n",
    "        # getTickFrequency() Returns the number of ticks per second. \n",
    "        label = 'Inference time: %.2f ms' % (retval * 1000.0 / cv2.getTickFrequency())\n",
    "        \n",
    "        #Add label to the frame\n",
    "        cv2.putText(frame, label, (0, 15), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255))\n",
    "\n",
    "        # Save frame with the detection boxes    \n",
    "        videoWriter.write(frame.astype(np.uint8)) \n",
    "        \n",
    "        #Add scene information\n",
    "        log_scene.append(information)\n",
    "    return log_scene\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def describe_video(video_output):\n",
    "    bucket = []\n",
    "    for index, frame in enumerate(video_output): \n",
    "        row = []\n",
    "        for item in range(0, len(frame)):\n",
    "            row.append(classes[video_output[index][item]])\n",
    "        bucket.append(row) \n",
    "    scene = pd.DataFrame(bucket)\n",
    "    scene = scene.dropna(how='all')\n",
    "    scene = scene.set_index(1).stack()\n",
    "    scene = scene.groupby(level=[0,1]).nunique(dropna=False).unstack(fill_value=0)\n",
    "    scene = scene.T    \n",
    "    return scene   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "TEST_DIR = 'test_videos'\n",
    "TEST_DIR_DETECTION = 'test_videos_detection'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load Classes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = pd.read_csv('class_names.csv')\n",
    "classes = classes['class'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['person',\n",
       " 'bicycle',\n",
       " 'car',\n",
       " 'motorbike',\n",
       " 'aeroplane',\n",
       " 'bus',\n",
       " 'train',\n",
       " 'truck',\n",
       " 'boat',\n",
       " 'traffic light']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes[:10] #Display a few classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Darknet-53 Takes input image of 416 x 416**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgWidth = 416       #Darknet-53 image Width \n",
    "imgHeight = 416      #Darknet-53 image Height\n",
    "score_threshold = 0.5  #Confidence threshold to determine whether to accept of reject\n",
    "nms_threshold = 0.4   #Non-maximum suppression threshold. Ensures that the object is only detected once"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We will use original weights of the YoloV3 training to conduct the testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model configuaration and model weigts. This are pretrained weights and associated configuration\n",
    "model_config = 'cfg/yolov3-spp.cfg';\n",
    "model_weights= 'weights/yolov3-spp.weights';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Video 1: Before Object Detection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video width='640' height='300' controls>\n",
       "  <source src='demo/thailand.mp4' type='video/mp4'>\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<video width='640' height='300' controls>\n",
    "  <source src='demo/thailand.mp4' type='video/mp4'>\n",
    "</video>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Video 1: After Object Detection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video width='640' height='300' controls>\n",
       "  <source src='demo/thailand_detection.mp4' type='video/mp4'>\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<video width='640' height='300' controls>\n",
    "  <source src='demo/thailand_detection.mp4' type='video/mp4'>\n",
    "</video>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## USE CASE : AI BASED VIDEO ANNOTATION AND RETRIEVAL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We have shown above that for a given video, the model can detect classes within the video. What if we saved the boundary box information, the class detected and time of detection etc into a database? Can we then search the database for videos that meet the specific criteria?**\n",
    "<br><br>\n",
    " **Example**\n",
    "<p>\n",
    "<table>\n",
    "    <thead>\n",
    "        <tr>          \n",
    "          <th>File Name</th>          \n",
    "          <th>File Location</th>\n",
    "          <th>Classes Detected</th>          \n",
    "    </tr>\n",
    "     </thead>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <td>video_01.mp4</td>\n",
    "            <td>data/videos</td>\n",
    "            <td>[person, dog, horse]</td>\n",
    "         <tr>\n",
    "           <td>video_01.mp4</td>\n",
    "           <td>data/videos</td>\n",
    "           <td>[bagpack, bottle]</td>\n",
    "         <tr>\n",
    "         <tr>\n",
    "           <td>video_03.mp4</td>\n",
    "           <td>data/videos</td>\n",
    "           <td>[car, truck, trafficlight]</td>\n",
    "         <tr>\n",
    "    </tbody>\n",
    "</table>\n",
    "\n",
    "**In the table above, we only show three variables, but the table can include metadata information from the video e.g. location video was shot, time/date of video was shotl, ength of video, Focal length, etc. Further, the task can be expanded to address activity tracking rather than object detection and those parameters can be added to the database.  With this information, we can answer the question, for example: \"Show me the dunks made by Michael Jordan in NBA season of 1995\"**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For our simple case study here, we are going to look into a file directory of saved videos and return any video that contains the searched material**\n",
    "- **Step 1 : For all the videos in the directory, if the video has not been added to the annotation database, we detect the objects in the video, and update the annotation database. In practice, this annotation job could be done in a big data environment using Apache Spark or equilavent environments**\n",
    "- **Step 2: We search the annotation library and return videos that meet the search criteria** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotate_videos(annnotation_df):\n",
    "    for originalVideo in tqdm(os.listdir(TEST_DIR)):                        \n",
    "        videosAlreadyAnnotated = annnotation_df['fname'].unique().tolist()\n",
    "        \n",
    "        if originalVideo in videosAlreadyAnnotated:\n",
    "            print('Video Already Annotated ', originalVideo)\n",
    "        else:\n",
    "            fpath_originalVideo = os.path.join(TEST_DIR, originalVideo)\n",
    "            detectionVideo = originalVideo[:-4]+'_detection.avi'\n",
    "            fpath_detectionVideo = os.path.join(TEST_DIR_DETECTION, detectionVideo) \n",
    "            \n",
    "            model = load_model(model_config, model_weights)\n",
    "            data = detect_object(model, imgWidth,imgHeight, fpath_originalVideo,\n",
    "                         fpath_detectionVideo, score_threshold, nms_threshold)\n",
    "            \n",
    "            scene = describe_video(video_output = data)\n",
    "            \n",
    "            row = pd.Series({'fname':originalVideo, 'floc': fpath_originalVideo,'classes':' '.join(list(scene.columns))})\n",
    "            annnotation_df = annnotation_df.append(row, ignore_index=True)            \n",
    "    annnotation_df.to_csv(ANNOTATION_PATH, index=False)\n",
    "    return annnotation_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "ANNOTATON_DIR = 'annotation_db'\n",
    "ANNOTATION_FNAME = 'annotation.csv'\n",
    "ANNOTATION_PATH = os.path.join(ANNOTATON_DIR, ANNOTATION_FNAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "\n",
    "def load_annotations(path = ANNOTATION_PATH):    \n",
    "    file = pathlib.Path(ANNOTATION_PATH)\n",
    "    if file.exists ():\n",
    "        annnotation_df = pd.read_csv(ANNOTATION_PATH)\n",
    "    else:\n",
    "        annnotation_df = pd.DataFrame(data= {'fname':[] , 'floc': [], 'classes': []})      \n",
    "    return  annnotation_df   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_df = load_annotations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dataframe before annotation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fname</th>\n",
       "      <th>floc</th>\n",
       "      <th>classes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [fname, floc, classes]\n",
       "Index: []"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotation_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1/5 [07:36<30:24, 456.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object Detection Completed for:  test_videos_detection/video_01_detection.avi\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|████      | 2/5 [13:47<21:32, 430.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object Detection Completed for:  test_videos_detection/video_04_detection.avi\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|██████    | 3/5 [17:37<12:20, 370.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object Detection Completed for:  test_videos_detection/video_05_detection.avi\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|████████  | 4/5 [28:53<07:42, 462.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object Detection Completed for:  test_videos_detection/video_02_detection.avi\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|██████████| 5/5 [36:27<00:00, 459.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object Detection Completed for:  test_videos_detection/video_03_detection.avi\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "annnotation_df =  annotate_videos(annnotation_df = annotation_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dataframe after annotation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fname</th>\n",
       "      <th>floc</th>\n",
       "      <th>classes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>video_01.mp4</td>\n",
       "      <td>test_videos/video_01.mp4</td>\n",
       "      <td>car motorbike person</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>video_04.mp4</td>\n",
       "      <td>test_videos/video_04.mp4</td>\n",
       "      <td>elephant person</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>video_05.mp4</td>\n",
       "      <td>test_videos/video_05.mp4</td>\n",
       "      <td>car truck</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>video_02.mp4</td>\n",
       "      <td>test_videos/video_02.mp4</td>\n",
       "      <td>backpack bench bottle car chair cup diningtabl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>video_03.mp4</td>\n",
       "      <td>test_videos/video_03.mp4</td>\n",
       "      <td>car cow horse person truck</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          fname                      floc  \\\n",
       "0  video_01.mp4  test_videos/video_01.mp4   \n",
       "1  video_04.mp4  test_videos/video_04.mp4   \n",
       "2  video_05.mp4  test_videos/video_05.mp4   \n",
       "3  video_02.mp4  test_videos/video_02.mp4   \n",
       "4  video_03.mp4  test_videos/video_03.mp4   \n",
       "\n",
       "                                             classes  \n",
       "0                               car motorbike person  \n",
       "1                                    elephant person  \n",
       "2                                          car truck  \n",
       "3  backpack bench bottle car chair cup diningtabl...  \n",
       "4                         car cow horse person truck  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annnotation_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def searchVideos(searchList, annnotation_df = annnotation_df):\n",
    "    def filterVideos(searchList, classes):\n",
    "        success = False\n",
    "        classesList = classes.split(' ')\n",
    "        if [x for x in classesList  if x in searchList]:\n",
    "            success = True\n",
    "        return success\n",
    "    \n",
    "    results = annnotation_df[annnotation_df['classes'].apply(lambda x: filterVideos(searchList, x))]    \n",
    "    if results.empty:\n",
    "        print('No Results found for:', ' or '.join(searchList))\n",
    "    else:\n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**With the annotation database update complete, we can search the database(dataframe) to get videos that match our search criteria**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fname</th>\n",
       "      <th>floc</th>\n",
       "      <th>classes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>video_01.mp4</td>\n",
       "      <td>test_videos/video_01.mp4</td>\n",
       "      <td>car motorbike person</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          fname                      floc               classes\n",
       "0  video_01.mp4  test_videos/video_01.mp4  car motorbike person"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Show videos containing 'motorbike'\n",
    "searchVideos(['motorbike'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Display Video to Confirm  results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video width='640' height='300' controls>\n",
       "  <source src='test_videos/video_01.mp4' type='video/mp4'>\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<video width='640' height='300' controls>\n",
    "  <source src='test_videos/video_01.mp4' type='video/mp4'>\n",
    "</video>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fname</th>\n",
       "      <th>floc</th>\n",
       "      <th>classes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>video_04.mp4</td>\n",
       "      <td>test_videos/video_04.mp4</td>\n",
       "      <td>elephant person</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          fname                      floc          classes\n",
       "1  video_04.mp4  test_videos/video_04.mp4  elephant person"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Show videos containing 'elephant'\n",
    "searchVideos(['elephant'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video width='640' height='300' controls>\n",
       "  <source src='test_videos/video_04.mp4' type='video/mp4'>\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<video width='640' height='300' controls>\n",
    "  <source src='test_videos/video_04.mp4' type='video/mp4'>\n",
    "</video>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion Yolo V3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We were able to test Yolov3 on random personal and internet videos from the internet. We did not have annotated video samples that contacted ground truth. So we were unable to test mAP for video.** \n",
    "\n",
    "**FUTURE WORK**\n",
    "\n",
    "- **Train the Yolov3 network with new classes. We were not to because of hardware limitation. The process of training the network took at average of 5 days**\n",
    "- **Study the perfomance of YOLOv3 on real time live data**\n",
    "- **Video activity tracking**\n",
    "- **Video captioning using RNN and NLP**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References:\n",
    "1. YOLOv3: An Incremental Improvement: https://arxiv.org/abs/1804.02767\n",
    "2. http://cocodataset.org/#home\n",
    "3. OpenCV: https://docs.opencv.org/4.1.0/\n",
    "4. Understanding YOLO: https://hackernoon.com/understanding-yolo-f5a74bbc7967"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
